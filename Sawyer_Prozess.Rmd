---
title: "Tom Sawyer vs Der Prozess"
author: "Teodor Petrič"
date: "19 5 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 0. Nameščanje programov (Packages)

Namestitev:
Če ste program(e) že namestili, lahko preskočite ta korak.

Znak # v programskem bloku (chunk) pomeni, da se ta vrstica ne izvaja.
Odstrani # če želite program namestiti.

```{r}
# # Programe, ki jih še nimate, lahko namestite tudi na ta način:
# install.packages("readtext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textplots")
# install.packages("tidyverse")

```

# 0. Programi

Najprej moramo zagnati programe, ki jih potrebujemo za načrtovano delo.

```{r}
library(readtext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(tidyverse)

```

# 1. Preberemo besedila

```{r}
txt = readtext("data/books/*.txt", encoding = "UTF-8")
txt

```

# 2. Ustvarimo korpus

Ustvarimo korpus ali jezikovno gradivo. Ukaz v programu "quanteda" je corpus().

```{r}
romane = corpus(txt)

```

Povzetek:

```{r}
povzetek = summary(romane)
povzetek

```

Podatke iz povzetka bi lahko uporabili npr. za izračun povprečne dolžine povedi v besedilih:

```{r}
povzetek %>% 
  group_by(Text) %>%
  mutate(dolzina_povedi = Tokens/Sentences)

```

Lahko bi tudi izračunali kazalnik slovarske raznolikosti v besedilih, tj. razmerje med različnimi (types) in pojavnicami (tokens), kar se angleščini imenuje "type token ratio" (ttr).

Razlikujemo med slovarskimi enotami (lemma), različnicami (types) in pojavnicami (tokens):

npr. nemški glagol "gehen" je slovarska enota, ki ima več različnic ali oblik (npr. gehe, gehst, geht, gehen, geht, ging, gingst, ... gegangen). 

Pojavnice: nekatere oblike glagola so pogostejše kot druge, nekatere pa se v izbranem besedilu ne pojavljajo.

```{r}
povzetek %>% 
  group_by(Text) %>% 
  mutate(ttr = Types/Tokens)

```

Program quanteda ima za ugotavljanje slovarske raznolikosti (lexical diversity) več možnosti, kar zahteva razcepitev besedil na manjše enote, tj. tokens (besede, ločila idr.). Za nekatere funkcije moramo ustvariti besedilno matriko (document frequency matrix, dfm).


# 3. Tokenizacija

Če želimo več izvedeti o besedilih, npr. katere besede se pojavljajo v besedilih, moramo najprej ustvariti seznam besedilnih enot (tj. besed, ločil idr.).

Iz gradiva izvlečemo besedne oblike (npr. s pomočjo presledkov).

Za tokenizacijo ima quanteda ukaz tokens().

```{r}
besede = tokens(romane)
head(besede)

```

# 4. Čiščenje

S seznama lahko izločimo "nebesede":

```{r}
besede = tokens(romane, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T)
head(besede)

```

Izločimo lahko tudi besede, ki za vsebinsko analizo niso zaželene ("stopwords").

V izbranih besedilih motijo tudi angleške besede, ki niso sestavni del nemških besedil.

concatenate = združi: c()

```{r}
stoplist_de = c(stopwords("de"), "dass", "Aligned", "by", "autoalignment", "Source", "Project", 
                "bilingual-texts.com", "fully", "reviewed")
besede = tokens_select(besede, pattern = stoplist_de, selection = "remove")

```


Naslednji seznam bomo uporabljali za ustvarjanje konkordance, tj. seznama sobesedil, v katerem se nahaja iskalni niz (npr. neka beseda).

```{r}
stoplist_en = c("Aligned", "by", "autoalignment", "Source", "Project", 
                "bilingual-texts.com", "fully", "reviewed")

# Obdržali bomo ločila
woerter = tokens(romane, remove_symbols = T, remove_numbers = T, remove_url = T)
# Odstranili bomo angleške besede na začetku besedil
woerter = tokens_select(woerter, pattern = stoplist_en, selection = "remove", padding = TRUE)

```


# 5. Kwic

Za ustvarjanje konkordanc ima program quanteda ukaz "kwic" (keyword in context).

Možno je iskati posamezne besede, besedne zveze, uporabljamo pa lahko tudi nadomestne znake (npr. *).

```{r}
kwic(woerter, pattern = c("Frau", "Herr"))

```

Konkordanco bomo pretvorili v podatkovno zbirko (data frame / tibble). Prednost je npr., da tako pridobimo imena stolpcev (tj. spremenljivk).

KWIC() ima več možnosti, npr. "case_insensitive = FALSE" razlikuje med velikimi in malimi črkami. Privzeta vrednost je "TRUE", tj. da tega ne razlikuje (tako kot Excel).

```{r}
(konkordanca = kwic(woerter, pattern = c("Frau", "Herr"), case_insensitive = FALSE) %>% 
  as_tibble()
)

```

Z ukazom count() lahko preštejemo, koliko pojavnic je KWIC našel.

```{r}
konkordanca %>% 
  count(keyword)

```

Poiskati želimo besede s pripono "-in" za samostalnike, ki označujejo ženska osebna imena (npr. Ärztin, Köchin, ...).

```{r}
(konkordanca2 <- kwic(woerter, pattern = c("*in"), case_insensitive = FALSE) %>% 
  as_tibble()
)

```

Žal vsebuje gornji seznam sobesedil veliko besednih oblik, ki niso ženska osebna imena (npr. ein, in, ...). Če želimo natančnejši seznam, moramo iskati na ustreznejši način, npr. z naborom nadomestnih znakov, tako imeovanih *regularnih izrazov* (regular expressions, "regex").

Na portalu **https://regex101.com/** lahko preizkušate in se učite regularnih izrazov.

Poizvedovanje s pomočjo regularnih izrazov: *in.

```{r}
(konkordanca2 <- kwic(woerter, pattern = "\\A[A-Z][a-z]+[^Eae]in\\b",
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
  filter(keyword != "Immerhin", 
         keyword != "Darin",
         keyword != "Termin",
         keyword != "Worin",
         keyword != "Robin",
         keyword != "Medizin",
         keyword != "Austin",
         keyword != "Mussein",
         keyword != "Benjamin",
         keyword != "Franklin")
)

```

Še drug primer uporabe regularnih izrazov
Poizvedovanje s pomočjo regex: Manjšalnice / Diminutive (-chen, -lein).
Katera manjšalna pripona prevladuje: -lein ali -chen ?

```{r}
(konkordanca3a <- kwic(woerter, "*lein",
                      valuetype = "glob", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
   count(keyword, sort = TRUE)
)

(konkordanca3b <- kwic(woerter, "*chen",
                      valuetype = "glob", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
   count(keyword, sort = T)
)

(konkordanca3 <- kwic(woerter, 
                      pattern = c("\\A[A-Z][a-z]*[^aäeiouürs]chen\\b",
                                  "[A-Z]*[^kl]lein\\b"),
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble() %>% 
  filter(keyword != "Welchen", 
         keyword != "Manchen",
         keyword != "Solchen",
         keyword != "Fräulein")
)

```

Poizvedovanje s pomočjo "regex": Frau + Priimek / Ime

Obvezno nastavimo case_insensitive = FALSE, saj naj program razlikuje med velikimi in malimi začetnicami.

```{r}
(konkordanca4 <- kwic(woerter, pattern = phrase("\\bFrau\\b ^[A-Z][^[:punct:]]"), 
                      valuetype = "regex", case_insensitive = FALSE) %>% 
  as_tibble()
)

```


# 6. Pogostnost

Besedilno-besedna matrika (dfm) je izhodišče za izračun in grafični prikaz več statističnih količin, npr. tudi pogostnosti besednih oblik v posameznih besedilih:

```{r}
matrika = dfm(besede, tolower = FALSE) # za zdaj obdržimo velike začetnice

# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)
matrika = dfm_select(matrika, selection = "remove", pattern = stoplist_de)
matrika
```

Program quanteda ima posebno funkcijo, ki sestavi seznam besednih oblik in njihove pogostnosti, tj. textstat_frequency().

```{r}
library(quanteda.textstats)
library(quanteda.textplots)

(pogostnost = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt"))
)

```

Seznam besednih oblik in njihove pogostnosti lahko razdelim na dva posebna seznama, in sicer s funkcijo filter().

```{r}
(pogost_tom = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt")) %>% 
  filter(group == "tom.txt")
)

(pogost_prozess = textstat_frequency(matrika, groups = c("prozess.txt", "tom.txt")) %>% 
  filter(group == "prozess.txt")
)

```

Glagoli rekanja in mišljenja: kateri so v izbranih besedilih pogostnejši?

```{r}
(sagen = pogostnost %>%
   filter(str_detect(feature, "^(ge)?sag*"))
)

(reden = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?rede*"))
)

(fragen = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?frag*"))
)

(antworten = pogostnost %>% 
    filter(str_detect(feature, "^(ge)?antwort*"))
)

(rufen = pogostnost %>% 
    filter(str_detect(feature, pattern = "^(ge)?ruf*", negate = FALSE)) %>% 
    filter(!str_detect(feature, "ruh|run|rum|rui|ruch"))
)

```

```{r}
verb1 = sagen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "sagen")

verb2 = reden %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "reden")

verb3 = fragen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "fragen")

verb4 = antworten %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "antworten")

verb5 = rufen %>% 
  group_by(group) %>% 
  summarise(freq = sum(frequency)) %>% 
  mutate(verb = "rufen")

```

Pet majhnih tabel lahko združimo v večjo tabelo, tj. s funkcijo rbind().

```{r}
glagoli = rbind(verb1, verb2, verb3, verb4, verb5)
glagoli

```

Še diagram:

```{r}
glagoli %>% 
  ggplot(aes(freq, verb, fill = verb)) +
  geom_col() +
  facet_wrap(~ group) +
  theme(legend.position = "none")

```

Tabelo lahko tudi prerazporedimo, npr. zaradi lažje primerjave besedil takole:

```{r}
glagoli %>% 
  pivot_wider(id_cols = verb, names_from = group, values_from = freq)

```


# 7. Kolokacije

Koleksemi = slovarske enote, ki se sopojavljajo.
Kolokacije = jezikovne prvine, ki se sopojavljajo.

Statistična opredelitev:
Če se dva izraza (npr. "dober dan") pojavljata bistveno pogosteje kot neposredna soseda, kakor bi naključno pričakovali, potem ju lahko obravnavamo kot kolokacijo.

Jezikoslovna opredelitev:
Kolokacija je pomensko povezano zaporedje besed.

Funkcija textstat_collocations() v programu quanteda.

"woerter" je seznam besednih oblik (padding = TRUE !), ki smo ga ustvarili zgoraj.

```{r}
(coll_2 = textstat_collocations(woerter, size = 2, tolower = TRUE) # naredi male črke !
)

```

Kolokacije s tremi členi.

```{r}
(coll_3 = textstat_collocations(woerter, size = 3, tolower = FALSE)
)

```


```{r}
(coll_4 = textstat_collocations(woerter, size = 4, tolower = FALSE)
)

```

Sopomenski vprašalnici "warum" in "wieso: s katerimi besednimi oblikami se sopojavljata?

```{r}
(sogar <- coll_2 %>% 
  filter(str_detect(collocation, "^warum"))
)

(nur <- coll_2 %>% 
  filter(str_detect(collocation, "^wieso"))
)

```

Kolokacija samostalniških izrazov. V nemščini imajo veliko začetnico.
Najprej bomo sestavili seznam besednih oblik z veliko začetnico (woerter_caps).
Potem lahko pridobimo seznam kolokacij (coll_caps2).

```{r}
woerter_caps = tokens_select(woerter, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE, 
                                padding = TRUE)

coll_caps2 = textstat_collocations(woerter_caps, size = 2, tolower = FALSE, min_count = 5)
head(coll_caps2, 100)

```

Ni smiselno upoštevati "Der + samostalnik" kot kolokacijo, saj se v nemščini velika večina samostalnikov pojavlja s členom.

Zato bomo člene "Der, Die, Das" in še nekaj besednih oblik na začetku stavka spremenili v "der, die , das", ....

```{r}
woerter_small = tokens_replace(woerter, 
                               pattern = c("Der","Die","Das","Des","Wollen","Im","Zum",
                                           "Kein","Jeden","Wenn","Als","Da","Aber","Und","Sehen"), 
                               replacement = c("der","die","das","des","wollen","im","zum",
                                               "kein","jeden","wenn","als","da","aber","und","sehen"))

woerter_caps = tokens_select(woerter_small, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE, 
                                padding = TRUE)

coll_caps2 = textstat_collocations(woerter_caps, size = 2, tolower = FALSE, min_count = 5)
head(coll_caps2, 100)

```


# 8. Lematizacija

Seznam slovarskih enot (lem) lahko naložimo z medmrežja na naš disk.

```{r}
# Preberi seznam slovarskih enot in pojavnic z diska
lemdict = read.delim2("data/lemmatization_de.txt", sep = "\t", encoding = "UTF-8", 
                      col.names = c("lemma", "word"), stringsAsFactors = F)

# Pretvori podatkovna niza v znakovna niza
lemma = as.character(lemdict$lemma) 
word = as.character(lemdict$word)

# Lematiziraj pojavnice v naših besedilih
lemmas <- tokens_replace(besede,
                             pattern = word,
                             replacement = lemma,
                             case_insensitive = TRUE, 
                             valuetype = "fixed")


```

Ustvarimo matriko s slovarskimi enotami (namesto pojavnic).

```{r}
matrika_lem = dfm(lemmas, tolower = FALSE) # za zdaj obdržimo velike začetnice

# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)
matrika_lem = dfm_select(matrika_lem, selection = "remove", pattern = stoplist_de)
matrika_lem

```

# 9. Besedni oblaček

```{r}
textplot_wordcloud(matrika_lem, comparison = TRUE, adjust = 0.3, color = c("darkblue","darkgreen"),
                   max_size = 4, min_size = 0.75, rotation = 0.5, min_count = 30, max_words = 250)

```

Lepši oblaček (za obe besedili skupaj).

```{r}
# install.packages("wordcloud2)
matrika_lem_prozess = matrika_lem[1,]

set.seed(1320)
library(wordcloud2)
topfeat <- as.data.frame(topfeatures(matrika_lem_prozess, 100))
topfeat <- rownames_to_column(topfeat, var = "word")
wordcloud2(topfeat)

```


```{r}
matrika_lem_tom = matrika_lem[2,]

set.seed(1320)
library(wordcloud2)
topfeat <- as.data.frame(topfeatures(matrika_lem_tom, 100))
topfeat <- rownames_to_column(topfeat, var = "word")
wordcloud2(topfeat)

```

