{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "knitr::opts_chunk$set(echo = TRUE)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 0. Nameščanje programov (Packages)\n",
                "\n",
                "Namestitev: Če ste program(e) že namestili, lahko preskočite ta korak.\n",
                "\n",
                "Znak \\# v programskem bloku (chunk) pomeni, da se ta vrstica ne izvaja. Odstrani\n",
                "\\# če želite program namestiti.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Programe, ki jih še nimate, lahko namestite tudi na ta način (odstranite #):\n",
                "# install.packages(\"readtext\")\n",
                "# install.packages(\"quanteda\")\n",
                "# install.packages(\"quanteda.textstats\")\n",
                "# install.packages(\"quanteda.textplots\")\n",
                "# install.packages(\"tidyverse\")\n",
                "# install.packages(\"wordcloud2\")\n",
                "# install.packages(\"tidytext\")\n",
                "# install.packages(\"udpipe\")\n",
                "# install.packages(\"janitor\")\n",
                "# install.packages(\"scales\")\n",
                "# install.packages(\"widyr\")\n",
                "# install.packages(\"syuzhet\")\n",
                "# install.packages(\"corpustools\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 0. Programi\n",
                "\n",
                "Najprej moramo zagnati programe, ki jih potrebujemo za načrtovano delo.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(readtext)\n",
                "library(quanteda)\n",
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "library(tidyverse)\n",
                "library(tidytext)\n",
                "library(wordcloud2)\n",
                "library(udpipe)\n",
                "library(janitor)\n",
                "library(scales)\n",
                "library(widyr)\n",
                "library(syuzhet)\n",
                "library(corpustools)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Preberemo besedila\n",
                "\n",
                "Besedilni datoteki odpremo s programom *readtext* z istoimensko funkcijo *readtext()*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "txt = readtext(\"data/books/translations/sawyer/*.txt\", encoding = \"UTF-8\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Odstranili bomo začetek obeh besedilnih nizov (v stolpcu \"text\"), ker niso sestavni del besedil.\n",
                "\n",
                "Uporabljamo več funkcij / ukazov:\n",
                "- *filter()* za izbiranje vrstice v podatkovnem nizu, \n",
                "- *separate()* za delitev stolpca \"text\" v dva nova stolpca, \n",
                "- *rbind()* za združitev obeh podatkovnih nizov (\"txt1\", \"txt2\") v enega (\"txt\") \n",
                "- *mutate()* in *str_squish()* za odstranjevanje nepotrebnih presledkov med besedami v stolpcu \"text\".\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Nemški prevod\n",
                "txt1 = txt %>% \n",
                "  filter(doc_id == \"tom_de.txt\") %>% \n",
                "  separate(text, \n",
                "           into = c(\"kolofon\", \"text\"), \n",
                "           sep = \"Source : Project Gutenberg      Die Abenteuer Tom Sawyers      Mark Twain      \") %>% \n",
                "  select(-kolofon) # izločimo stolpec\n",
                "\n",
                "# Angleški izvirnik\n",
                "txt2 = txt %>% \n",
                "  filter(doc_id == \"tom_en.txt\") %>% \n",
                "  separate(text, \n",
                "           into = c(\"kolofon\", \"text\"), \n",
                "           sep = \"High up in Society\n",
                "\n",
                "Contentment\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\") %>% \n",
                "  separate(text, into = c(\"text\", \"project\"), \n",
                "           sep = \"\\\\*\\\\*\\\\* END OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF TOM SAWYER \\\\*\\\\*\\\\*\") %>% \n",
                "  select(-kolofon, -project) # izločimo stolpca\n",
                "\n",
                "# Obe datoteki združimo\n",
                "txt = rbind(txt1,txt2)\n",
                "# txt = txt %>% mutate(text = str_squish(text))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Ustvarimo korpus\n",
                "\n",
                "Ustvarimo korpus ali jezikovno gradivo. Ukaz v programu *quanteda* je *corpus()*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "romane = corpus(txt)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Povzetek korpusa:\n",
                "Program quanteda ima dve funkciji za povzemanje osnovnih vrednosti besedil: \n",
                "- *summary()* \n",
                "- *textstat_summary()*\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "romanstatistik = textstat_summary(romane)\n",
                "romanstatistik %>% rmarkdown::paged_table() # za lepši izpis v formatu html\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Iz evidence razberemo, da je nekaj razlik med nemškim prevodom in angleškim izvirnikom glede števila znakov (chars), povedi (\"sents\"), pojavnic (\"tokens\"), različnic ali besednih oblik (\"types\"), ločil (\"puncts\") in števk (\"numbers\").\n",
                "\n",
                "Podatke iz povzetka bi lahko uporabili npr. za izračun povprečne dolžine povedi\n",
                "v besedilih:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(rmarkdown)\n",
                "romanstatistik %>% \n",
                "  select(-tags, -emojis) %>% # izločimo dva prazna stolpca\n",
                "  group_by(document) %>%\n",
                "  mutate(dolzina_povedi = tokens/sents) %>% paged_table()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Izračun pokaže, da je dolžina povedi (tj. število besed na poved) večja kot npr. v sproščenem (zasebnem) ustnem dvogovoru o vsakdanji, manj zahtevni temi.\n",
                "\n",
                "Lahko bi tudi izračunali kazalnik slovarske raznolikosti v besedilih, tj.\n",
                "razmerje med različnimi (*types*) in pojavnicami (*tokens*), kar se angleščini\n",
                "imenuje \"type token ratio\" (*ttr*).\n",
                "\n",
                "Razlikujemo med slovarskimi enotami (*lemma*), različnicami (*types*) in pojavnicami (*tokens*):\n",
                "\n",
                "npr. nemški glagol \"gehen\" je slovarska enota, ki ima več različnic ali oblik\n",
                "(npr. gehe, gehst, geht, gehen, geht, ging, gingst, ... gegangen).\n",
                "\n",
                "Pojavnice: nekatere oblike glagola so pogostejše kot druge, nekatere pa se v\n",
                "izbranem besedilu ne pojavljajo.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "romanstatistik %>% \n",
                "  select(-tags, -emojis, -urls) %>% # izločimo stolpce iz prikaza\n",
                "  group_by(document) %>%\n",
                "  mutate(dolzina_povedi = round(tokens/sents, 2)) %>% # zaokroževanje \"round()\"\n",
                "  mutate(ttr = round(types/tokens, 4)) %>% paged_table()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Vrednost slovarske raznolikosti (ttr) je (glede na zmerno dolžino besedila) razmeroma nizka, mogoče znamenje, da je besedilo napisano v vsakdanjem pogovornem slogu.\n",
                "\n",
                "Program *quanteda* ima za ugotavljanje slovarske raznolikosti (*lexical diversity*) več možnosti, kar zahteva razcepitev besedil na manjše enote, tj. *tokens* (besede, ločila idr.). Za nekatere funkcije moramo ustvariti besedilno matriko (document frequency matrix, *dfm*).\n",
                "\n",
                "\n",
                "# 3. Tokenizacija\n",
                "\n",
                "Če želimo več izvedeti o besedilih, npr. katere besede se pojavljajo v besedilih, moramo najprej ustvariti seznam besedilnih enot (tj. besed, ločil idr.).\n",
                "\n",
                "Iz gradiva izvlečemo besedne oblike (npr. s pomočjo presledkov).\n",
                "\n",
                "Za tokenizacijo ima quanteda ukaz tokens().\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "besede = tokens(romane)\n",
                "head(besede)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Čiščenje\n",
                "\n",
                "S seznama lahko izločimo \"nebesede\":\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "besede = tokens(romane, remove_punct = T, remove_symbols = T, remove_numbers = T, remove_url = T)\n",
                "head(besede)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Izločimo lahko tudi besede, ki za vsebinsko analizo niso zaželene (\"stopwords\").\n",
                "\n",
                "concatenate = združi: *c()*\n",
                "\n",
                "Seznamoma smo dodali še nekaj besed (funkcijske besede ali zelo pogoste glagolske oblike).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stoplist_de = c(stopwords(\"de\"), \n",
                "                \"dass\", \"s\", \"Na\", \"wurde\", \"ganz\", \"immer\", \"sagte\", \"mehr\",\n",
                "                \"schon\", \"ja\", \"mal\", \"ne\", \"n\", \"wohl\", \"sagen\", \"gar\")\n",
                "stoplist_en = c(stopwords(\"en\"), \"now\", \"one\", \"got\", \"upon\", \"just\", \"said\",\n",
                "                \"Well\", \"Oh\", \"ever\", \"around\", \"made\", \"say\", \"Project\")\n",
                "stoplist = c(stoplist_de, stoplist_en)\n",
                "\n",
                "besede = tokens_select(besede, pattern = stoplist, selection = \"remove\",\n",
                "                       padding = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Naslednji seznam bomo uporabljali za ustvarjanje konkordance, tj. seznama\n",
                "sobesedil, v katerem se nahaja iskalni niz (npr. neka beseda).\n",
                "\n",
                "Pri odstranjevanju nezaželenih izrazov je nastavljena opcija *padding = TRUE*. Na mestu nezaželenega izraza bo ostal znak dolžine nič (\"\"). To je pomembno pri iskanju večbesednih zvez (kolokacij).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Obdržali bomo ločila\n",
                "woerter = tokens(romane, remove_symbols = T, remove_numbers = T, remove_url = T)\n",
                "woerter = tokens_select(woerter, pattern = stoplist, selection = \"remove\",\n",
                "                       padding = TRUE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 5. Kwic\n",
                "\n",
                "Za sestavo konkordanc ima program quanteda funkcijo *kwic()* (keyword in\n",
                "context).\n",
                "\n",
                "Možno je iskati posamezne besede, besedne zveze, uporabljamo pa lahko tudi\n",
                "nadomestne znake (npr. \\*).\n",
                "\n",
                "*kwic()* ima več možnosti, npr. \"case_insensitive = FALSE\" razlikuje med\n",
                "velikimi in malimi črkami. Privzeta vrednost je \"TRUE\" (tako kot Excel).\n",
                "\n",
                "Konkordanco bomo pretvorili v podatkovno zbirko, tj. *data.frame* ali\n",
                "*tibble()*. Prednost je npr., da tako pridobimo imena stolpcev (tj.\n",
                "spremenljivk).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "konkordanca = kwic(woerter, \n",
                "                   pattern = c(\"Tom\", \"Sawyer\", \"Huck\", \"Huckleberry\", \"Finn\"), \n",
                "                   case_insensitive = FALSE,\n",
                "                   window = 10) %>% \n",
                "  as_tibble()\n",
                "\n",
                "head(konkordanca) %>% paged_table()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Z ukazom *count()* lahko preštejemo, koliko pojavnic je *kwic()* našel.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "konkordanca %>% \n",
                "  count(keyword, sort = TRUE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Skladno z našim pričakovanjem sta \"Tom\" in \"Huck\", glavna junaka romana, pogosta  izraza. Natančneje si lahko ogledamo tudi sobesedilo, v katerem se pojavljajo iskani znakovni nizi (keywords).\n",
                "\n",
                "\n",
                "# 6. Pogostnost\n",
                "\n",
                "Besedilno-besedna matrika (dfm) je izhodišče za izračun in grafični prikaz več\n",
                "statističnih količin, npr. tudi pogostnosti besednih oblik v posameznih\n",
                "besedilih:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "matrika = dfm(besede, tolower = FALSE) # za zdaj obdržimo velike začetnice\n",
                "\n",
                "# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)\n",
                "matrika = dfm_select(matrika, selection = \"remove\", pattern = stoplist)\n",
                "matrika\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Program *quanteda* ima posebno funkcijo, ki sestavi seznam besednih oblik in\n",
                "njihove pogostnosti, tj. *textstat_frequency()*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(quanteda.textstats)\n",
                "library(quanteda.textplots)\n",
                "\n",
                "pogostnost = textstat_frequency(matrika, groups = c(\"tom_de.txt\", \"tom_en.txt\"))\n",
                "head(pogostnost)\n",
                "tail(pogostnost)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Na diagramu najpogostnejših izrazov v izvirniku in prevodu lahko zasledimo podobne tendence in razlike, nazorno pa nam pokaže tudi, ali smo izločili vse tiste izraze, ki jih za vsebinsko analizo ne želimo imeti na seznamu in ali bi bilo smiselno, dopolniti seznam \"stoplist\" (gl. zgoraj).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "as_tibble(pogostnost) %>%\n",
                "  slice_max(order_by = frequency, n = 60) %>%\n",
                "  mutate(feature = reorder_within(feature, frequency, frequency, sep = \": \")) %>%\n",
                "  # ggplot(aes(frequency, reorder(feature, frequency))) +\n",
                "  ggplot(aes(frequency, feature)) +\n",
                "  geom_col(fill=\"steelblue\") +\n",
                "  labs(x = \"Frequency\", y = \"\") +\n",
                "  facet_wrap(~ group, scales = \"free_y\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 7. Kolokacije\n",
                "\n",
                "*Koleksemi* = slovarske enote, ki se sopojavljajo. \n",
                "*Kolokacije* = jezikovne prvine, ki se sopojavljajo.\n",
                "\n",
                "*Statistična opredelitev*: Če se dva izraza (npr. \"dober dan\") pojavljata bistveno\n",
                "pogosteje kot neposredna soseda, kakor bi naključno pričakovali, potem ju lahko\n",
                "obravnavamo kot kolokacijo.\n",
                "\n",
                "*Jezikoslovna opredelitev*: Kolokacija je pomensko povezano zaporedje besed.\n",
                "\n",
                "Pomembno: za ugotavljanje kolokacij potrebujemo besedni seznam z opcijo *padding = TRUE* ! V besednem seznamu \"woerter\" smo sicer izločili nezaželene besedne oblike, ampak opcija padding = TRUE namesto izločenih besed vstavi vrzel oz. prazen niz \"\". Tako program prepreči odkrivanje lažnih kolokacij.\n",
                "\n",
                "Funkcija *textstat_collocations()* programa *quanteda* nam bo poiskala (statistično opredeljene) kolokacije. Z opcijo *size* nastavimo, koliko členov naj vsebuje (npr. 2 za dve besedni obliki, 2:3 za dve ali tri besede). Opcija *tolower = TRUE* odpravi razlikovanje med malimi in velikimi črkami. Opcija *minimal_count* določa, kolikšna naj bo najmanjša pogostnost. \n",
                "\n",
                "V naslednjih preglednicah so prikazane kolokacije obeh romanov, izvirnika in prevoda.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coll_2 = textstat_collocations(woerter, # seznma besednih oblik\n",
                "                               size = 2, # obseg kolokacije\n",
                "                               tolower = TRUE,  # naredi male črke !\n",
                "                               min_count = 2) # prag pogostnosti\n",
                "head(coll_2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tročlenskih kolokacij je precej manj kot dvočlenskih.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coll_3 = textstat_collocations(woerter, size = 3, tolower = TRUE, \n",
                "                               min_count = 2)\n",
                "head(coll_3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Program ni našel štričlenskih kolokacij, ki bi se pojavljale vsaj dvakrat. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coll_4 = textstat_collocations(woerter, size = 4, tolower = TRUE, \n",
                "                               min_count = 2)\n",
                "head(coll_4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Seznam vseh kolokacij velikost 2, 3 in 4. V stolpcu *count_nested* program šteje kolokacije, vsebovane v drugi kolokaciji (višjega reda). \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "coll_2_4 = textstat_collocations(woerter, size = 2:4, tolower = TRUE, \n",
                "                               minimal_count = 2)\n",
                "head(coll_2_4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Kolokacija samostalniških izrazov. \n",
                "\n",
                "V nemščini imajo samostalniki veliko začetnico. Najprej bomo sestavili seznam besednih oblik z veliko začetnico (woerter_caps). Pri tem nam pomagata *regularni izraz* \"^[A-Z]\" in opcija *case_insensitive = FALSE*. Potem lahko pridobimo seznam kolokacij (coll_caps2).\n",
                "\n",
                "Spremenljivki *lambda* in *z* nam povesta, kako značilna je kolokacija v besedilu.\n",
                "\n",
                "Najprej kolokacije v nemškem prevodu, ki so sestavljene iz besednih oblik z veliko začetnico (poleg lastnih imen tudi splošna imena): \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# seznam besed z veliko začetnico\n",
                "woerter_caps_de = tokens_select(woerter[\"tom_de.txt\"], \n",
                "                                pattern = \"^[A-Z]\", \n",
                "                                valuetype = \"regex\", \n",
                "                                case_insensitive = FALSE, \n",
                "                                padding = TRUE)\n",
                "\n",
                "# kolokacije besed z veliko začetnico\n",
                "coll_caps2_de = textstat_collocations(woerter_caps_de, size = 2, tolower = FALSE,\n",
                "                                      min_count = 5)\n",
                "head(coll_caps2_de, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Še kolokacije v angleškem izvirniku, ki so sestavljene le iz lastnih imen:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "woerter_caps_en = tokens_select(woerter[\"tom_en.txt\"], \n",
                "                                pattern = \"^[A-Z]\", \n",
                "                                valuetype = \"regex\", \n",
                "                                case_insensitive = FALSE, \n",
                "                                padding = TRUE)\n",
                "\n",
                "\n",
                "coll_caps2_en = textstat_collocations(woerter_caps_en, size = 2, tolower = FALSE,\n",
                "                                      min_count = 5)\n",
                "head(coll_caps2_en, 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 8. Lematizacija\n",
                "\n",
                "*Slovarska enota* (*lema*) je osnovna oblika neke besede (geslo v slovarju): imenovalnik ednine, če gre za samostalniško obliko oz. nedoločnik, če gre za glagolsko obliko itd. \n",
                "\n",
                "Seznam slovarskih enot lahko sestavimo sami, bistveno hitreje (čeprav ne brez napak!) pa to opravimo programsko, npr. z *udpipe* ali *spacyr*. \n",
                "\n",
                "\n",
                "## Lasten seznam\n",
                "Seznam slovarskih enot (lem) lahko naložimo z medmrežja na naš disk. \n",
                "\n",
                "Tu je prikazan postopek za nemški prevod. Naš *quanteda* korpus vsebuje tudi angleško besedilo, ki ga moramo izločiti, preden začnem lematizacijo nemških besednih oblik.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "besede_de = besede[\"tom_de.txt\"]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Če imamo primeren seznam na disku, je postopek za uporabo s korpusom *quanteda* npr. takšen:\n",
                "- odpremo datoteko, ki vsebuje seznam lem (npr. z ukazom *read.delim2()* - odvisno od datotečne oblike);\n",
                "- za uporabo s korpusom pretvorimo stolpca podatkovnega niza v besedna seznama (tj. *as.character()*);\n",
                "- nazadnje zamenjamo besedne oblike z lemami(s funkcijo *token_replace()*). Če ustrezne leme ne najde, obdrži besedno obliko, ki jo je program našel v besedilu.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preberi seznam slovarskih enot in pojavnic z diska\n",
                "lemdict = read.delim2(\"data/lemmatization_de.txt\", \n",
                "                      sep = \"\\t\", # stolpci so ločeni tabulatorsko\n",
                "                      encoding = \"UTF-8\", # univerzalno kodiranje črk\n",
                "                      col.names = c(\"lemma\", \"word\"), # dodamo imena stolpcev\n",
                "                      stringsAsFactors = F) # preberi kot črkovne nize\n",
                "\n",
                "# Pretvori podatkovna niza v znakovna niza\n",
                "lemma = as.character(lemdict$lemma) # v tem stolpcu je osnovna oblika besede\n",
                "word = as.character(lemdict$word) # v tem stolpcu je ena izmed besednih oblik\n",
                "\n",
                "# Lematiziraj pojavnice v naših besedilih\n",
                "lemmas_de <- tokens_replace(besede_de, # seznam nemških besednih oblik (tokens)\n",
                "                             pattern = word, # obliko, ki jo želimo zamenjati\n",
                "                             replacement = lemma, # zamenjava\n",
                "                             case_insensitive = TRUE, # ne glede na začetnico\n",
                "                             valuetype = \"fixed\") # natančno ujemanje oblik\n",
                "\n",
                "lemmas_de # zdaj imamo leme (če je program našel zamenjavo za besedno obliko)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Zdaj ko imamo seznam slovarskih enot, lahko ustvarimo tudi matriko s slovarskimi enotami (namesto s pojavnicami), in sicer z funkcijo *dfm()* tako kot zgoraj.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "matrika_lem_de = dfm(lemmas_de, \n",
                "                     tolower = FALSE) # za zdaj obdržimo velike začetnice\n",
                "\n",
                "# Odstranimo besede, ki jih v vsebinski analizi ne potrebujemo (stopwords)\n",
                "matrika_lem_de = dfm_select(matrika_lem_de, \n",
                "                            selection = \"remove\", \n",
                "                            pattern = stoplist_de)\n",
                "matrika_lem_de\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Udpipe\n",
                "\n",
                "### Angleški izvirnik\n",
                "\n",
                "Lematizacijo angleškega izvirnika bomo opravili s programom *udpipe*, ki je na voljo za številne jezike (tudi slovenščino).\n",
                "\n",
                "Pred prvo uporabo moramo naložiti model za nemški jezik z interneta.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install.packages(\"udpipe)\n",
                "library(udpipe)\n",
                "language_model <- udpipe_download_model(language = \"english\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "V naslednjem koraku naložimo jezikovni model v pomnilnik.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ud_en <- udpipe_load_model(language_model$file_model)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Če je jezikovni model že v naši delovni mapi, download ni potreben, saj ga lahko\n",
                "takoj naložimo z diska v pomnilnik.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_model = \"english-ewt-ud-2.5-191206.udpipe\"\n",
                "ud_en <- udpipe_load_model(file_model)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Naslednji korak je *udpipe_annotate()*: program udpipe označuje besedne oblike\n",
                "po več merilih. Lematizacijo je le ena izmed nalog, ki jih program opravi.\n",
                "\n",
                "Udpipe prebere in označuje besedilo takole:\n",
                "\n",
                "Na začetku je *readtext()* prebral besedila, shranili smo jih pod imenom \"txt\". Angleški izvirnik smo shranili pod imenom \"txt2\", besedilo pa je v stolpcu \"text\".\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x <- udpipe_annotate(ud_en, # jezikovni model\n",
                "                     x = txt2$text,  # izbran je le angleški izvirnik\n",
                "                     trace = TRUE) # sledimo napredku anotacije\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pretvorba seznama v podatkovni niz s funkcijo *as.data.frame()*:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Alternativno branje angleškega izvirnika\n",
                "# # samo drugo besedilo:\n",
                "# x <- udpipe_annotate(ud_en, x = txt$text[2], trace = TRUE)\n",
                "en_df <- as.data.frame(x)\n",
                "head(en_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Nemški prevod\n",
                "\n",
                "Lematizacijo nemškega prevod bomo tokrat opravili s programom *udpipe*.\n",
                "\n",
                "Pred prvo uporabo moramo naložiti model za nemški jezik z interneta.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# install.packages(\"udpipe)\n",
                "library(udpipe)\n",
                "sprachmodell <- udpipe_download_model(language = \"german\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "V naslednjem koraku naložimo jezikovni model v pomnilnik.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ud_de <- udpipe_load_model(sprachmodell$file_model)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Če je jezikovni model že v naši delovni mapi, download ni potreben, saj ga lahko\n",
                "takoj naložimo z diska v pomnilnik.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "file_model = \"german-gsd-ud-2.5-191206.udpipe\"\n",
                "ud_de <- udpipe_load_model(file_model)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Naslednji korak je *udpipe_annotate()*: program udpipe označuje besedne oblike\n",
                "po več merilih. Lematizacijo je le ena izmed nalog, ki jih program opravi.\n",
                "\n",
                "Udpipe prebere in označuje besedilo takole:\n",
                "\n",
                "Na začetku je *readtext()* prebral besedila, shranili smo jih pod imenom \"txt\". Nemški prevod smo shranili pod imenom \"txt1\", besedilo pa je v stolpcu \"text\".\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x <- udpipe_annotate(ud_de, # jezikovni model\n",
                "                     x = txt1$text,  # izbran je le nemški prevod romana\n",
                "                     trace = TRUE) # sledimo napredku anotacije\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pretvorba seznama v podatkovni niz s funkcijo *as.data.frame()*:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Alternativno branje angleškega izvirnika\n",
                "# # samo drugo besedilo:\n",
                "# x <- udpipe_annotate(ud_en, x = txt$text[2], trace = TRUE)\n",
                "de_df <- as.data.frame(x)\n",
                "head(de_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 9. Besedni oblaček\n",
                "\n",
                "Besedni oblački so smiseln in razmeroma preprost prikaz najpogostejših besed v besedilu. Največkrat jih uporabljajo za prikaz vsebinsko relevantnih besed. Zato je treba najprej odstraniti funkcijske in druge neprimerne izraze. Še boljši pregled nad vsebino besedila nam besedni oblački dajejo, če uporabljamo slovarske enote (leme) namesto besednih oblik. To še posebej velja v oblikoslovno bogatih jezikih kot sta slovenščino in nemščina.\n",
                "\n",
                "Podatkovni niz \"en_df\", ki ga je ustvaril *udpipe*, moramo pripraviti za program *wordcloud2*:\n",
                "- izločiti nezaželene izraze,\n",
                "- ugotoviti pogostnost besed in\n",
                "- omejiti število besed za prikaz v besednem oblačku.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "en_df_ud <- en_df %>% \n",
                "  filter(upos != \"PUNCT\") %>% # izločimo ločila\n",
                "  filter(str_detect(lemma, \"^[:alpha:]\")) %>% # samo črke, ne simobolov itd.\n",
                "  mutate(word = str_to_lower(lemma)) # vse pretvorimo v male črke\n",
                "\n",
                "# iz besednega senzama naredimo podatkovni niz\n",
                "stoplist_eng = as_tibble(stoplist_en) %>% \n",
                "  rename(word = value) # sprememba imena\n",
                "\n",
                "# odstranimo nezaželene besede\n",
                "en_df_cleaned = en_df_ud %>% \n",
                "  anti_join(stoplist_eng, by = \"word\")\n",
                "\n",
                "# preštejemo besede in izberemo najpogostejše\n",
                "topfeat_en = en_df_cleaned %>% \n",
                "  count(word, sort = TRUE) %>% \n",
                "  head(300) %>% \n",
                "  as_tibble()\n",
                "\n",
                "# Oblaček\n",
                "set.seed(1320)\n",
                "library(wordcloud2)\n",
                "wordcloud2(topfeat_en)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Oblaček nemških slovarskih enot:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "de_df_ud <- de_df %>% \n",
                "  filter(upos != \"PUNCT\") %>% # brez ločil\n",
                "  filter(str_detect(lemma, \"^[:alpha:]\")) %>% # samo črke, ne simobolov itd.\n",
                "  mutate(word = str_to_lower(lemma)) # vse pretvorimo v male črke\n",
                "\n",
                "# iz besednega seznama naredimo podatkovni niz\n",
                "stoplist_deu = as_tibble(stoplist_de) %>% rename(word = value)\n",
                "\n",
                "# odstranimo nezaželene besede\n",
                "de_df_cleaned = de_df_ud %>% \n",
                "  anti_join(stoplist_deu, by = \"word\")\n",
                "\n",
                "# preštejemo besede, zadnji popravki in izberemo najpogostejše\n",
                "topfeat_de = de_df_cleaned %>% \n",
                "  count(word, sort = TRUE) %>% \n",
                "  filter(!str_detect(word, \"er\\\\|es\\\\|sie\")) %>% # izločimo z regularnim izrazom\n",
                "  filter(!str_detect(word, \"sie\\\\|sie\")) %>% # izločimo z regex\n",
                "  mutate(word = str_replace(word, \"hucken\", \"huck\")) %>% # popravek !!!\n",
                "  head(300) %>% \n",
                "  as_tibble()\n",
                "\n",
                "# Oblaček\n",
                "set.seed(1320)\n",
                "library(wordcloud2)\n",
                "wordcloud2(topfeat_de)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Oblaček angleških slovarskih enot, ki smo jih pridobili s programom *udpipe*, lahko tudi pripravimo za prikaz s funkcijo *textplot_wordcloud()* programa *quanteda*. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tok_en = en_df_cleaned %>% \n",
                "  select(word) %>% \n",
                "  mutate(word = paste(word, collapse = \" \")) %>% \n",
                "  head(1)\n",
                "toks_en = tokens(tok_en$word)\n",
                "\n",
                "matrika_lem_en = dfm(toks_en)\n",
                "matrika_lem_en = dfm_select(matrika_lem_en, \n",
                "                            pattern = stoplist_en, \n",
                "                            selection = \"remove\")\n",
                "\n",
                "# spremenimo ime (doc_id)\n",
                "docnames(matrika_lem_en) <- \"tom_en\"\n",
                "\n",
                "textplot_wordcloud(matrika_lem_en, # le nemški prevod\n",
                "                   comparison = FALSE, # brez primerjave z drugim besedilom\n",
                "                   adjust = 0.025, \n",
                "                   color = c(\"darkblue\",\"orange\",\"darkgreen\"),\n",
                "                   max_size = 5, min_size = 0.75, rotation = 0.5, \n",
                "                   min_count = 10, # spodnji prag pogostnosti\n",
                "                   max_words = 250) # koliko besed sme biti v oblačku\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Priprava seznama nemških slovarskih enot, ki smo jih pridobili z *udpipe*, in prikaz s funkcijo textplot_wordcloud().\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tok_de = de_df_cleaned %>% \n",
                "  select(word) %>% \n",
                "  mutate(word = paste(word, collapse = \" \")) %>% \n",
                "  head(1)\n",
                "toks_de = tokens(tok_de$word)\n",
                "\n",
                "matrika_lem_de = dfm(toks_de)\n",
                "matrika_lem_de = dfm_select(matrika_lem_de, \n",
                "                            pattern = c(stoplist_de, \"|\"), \n",
                "                            selection = \"remove\")\n",
                "\n",
                "# spremenimo ime (doc_id)\n",
                "docnames(matrika_lem_de) <- \"tom_de\"\n",
                "\n",
                "textplot_wordcloud(matrika_lem_de, # le nemški prevod\n",
                "                   comparison = FALSE, # brez primerjave z drugim besedilom\n",
                "                   adjust = 0.025, \n",
                "                   color = c(\"darkblue\",\"orange\",\"darkgreen\"),\n",
                "                   max_size = 5, min_size = 0.75, rotation = 0.5, \n",
                "                   min_count = 10, # spodnji prag pogostnosti\n",
                "                   max_words = 250) # koliko besed sme biti v oblačku\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Združimo matriki s funkcijo *rbind()*.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "matrika_lem_de_en = rbind(matrika_lem_de, matrika_lem_en)\n",
                "matrika_lem_de_en\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Če želimo, lahko matriko pretvorimo v podatkovni niz: \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "convert(matrika_lem_de_en, to = \"data.frame\") %>%\n",
                "  write_csv(\"data/tom_tom_matrika.csv\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Primerjalni oblaček nemških in angleških slovarskih enot:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textplot_wordcloud(matrika_lem_de_en, \n",
                "                   comparison = TRUE, # primerjava z drugim besedilom\n",
                "                   adjust = 0.025, \n",
                "                   color = c(\"darkblue\",\"darkgreen\"),\n",
                "                   max_size = 4, min_size = 0.5, rotation = 0.5, \n",
                "                   min_count = 10, # spodnji prag pogostnosti\n",
                "                   max_words = 120) # koliko besed sme biti v oblačku\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 10. Položaj v besedilu (xray)\n",
                "\n",
                "Diagram prikazuje, kje v besedilih se pojavlja določena besedna oblika. Podobno:\n",
                "Voyant Tools (MicroSearch).\n",
                "\n",
                "Za primerjavo so bili izbrani izrazi, ki dandanes niso več nevtralni, temveč bolj ali manj rasistično obarvani ali celo pejorativni.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "kwic_tom = kwic(besede, \n",
                "                 pattern = c(\"indian*\", \"injun\", # indinaer?\n",
                "                             \"neg*\", \"nigg*\")) # neger?\n",
                "textplot_xray(kwic_tom)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 11. Slovarska raznolikost\n",
                "\n",
                "Za oceno slovarske raznolikosti besedil je več meril. Najosnovnejša in najbrž najbolj znano je razmerje med številom različnic in pojavnic (TTR). Slaba lastnost tega merila je odvisnost od velikosti besedila.\n",
                "\n",
                "Program *quanteda* nam s funkcijo *textstat_lexdiv()* pričara celo paleto meril za slovarsko raznolikost (več o njih v pomoči programa).\n",
                "\n",
                "V spodnji razpredelnici vidimo številke po odstranitvi funkcijskih besed in nekaterih drugih nezaželenih izrazov (stopwords). TTR nemškega prevoda je večji kot tisti za angleški izvirnik, kar bi lahko pomenilo, da vsebuje več oblik.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textstat_lexdiv(matrika, measure = \"all\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "V naslednji tabeli vidimo izračun slovarske raznolikosti na osnovi slovarskih enot (namesto različnic).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textstat_lexdiv(matrika_lem_de_en, measure = \"all\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 12. Podobnost besedil\n",
                "\n",
                "Ta postopek je bolj zanimiv, če želimo primerjati več besedil. Zato bomo dodali\n",
                "še Kafkino novelo.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# odpremo datoteko\n",
                "verwandl = readtext(\"data/books/verwandlung/verwandlung.txt\", encoding = \"UTF-8\")\n",
                "# ustvarimo nov korpus\n",
                "verw_corp = corpus(verwandl)\n",
                "# združimo novi korpus s prrejšnjim\n",
                "romane3 = romane + verw_corp\n",
                "# tokenizacija\n",
                "romane3_toks = tokens(romane3)\n",
                "# ustvarimo matriko (dfm)\n",
                "romane3_dfm = dfm(romane3_toks)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Rezultat (ki je bil pričakovan): Kafkina novela \"Die Verwandlung\" je nemškemu prevoda podobnejši kot angleški izvirnik Twainovega romana \"Tom Sawyer\". Program očitno ne primerja vsebine besedil, temveč besedne oblike.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textstat_simil(romane3_dfm, method = \"cosine\", margin = \"documents\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Podobnost oblik (features).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# compute some term similarities\n",
                "simil1 = textstat_simil(matrika, matrika[, c(\"Tom\", \"Sawyer\", \"Huck\", \"Finn\")], \n",
                "                         method = \"cosine\", margin = \"features\")\n",
                "head(as.matrix(simil1), 10)\n",
                "tail(as.matrix(simil1), 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Različnost besedil (Kaj je ta metoda upoštevala? Razliko v dolžini?):\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot a dendrogram after converting the object into distances\n",
                "dist1 = textstat_dist(romane3_dfm, method = \"euclidean\", margin = \"documents\")\n",
                "plot(hclust(as.dist(dist1)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 13. Ključne besede\n",
                "\n",
                "Katere besedne oblike lahko uvrstimo med ključne besede, tj. take izraze, ki so\n",
                "najbolj značilni za neko besedilo? Program quanteda ima funkcijo\n",
                "*textstat_keyness()*: ciljno besedilo (target) primerjamo z referenčnim\n",
                "besedilom (reference).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "key_tom_de <- textstat_keyness(matrika, target = \"tom_de.txt\")\n",
                "key_tom_de\n",
                "\n",
                "key_tom_en <- textstat_keyness(matrika, target = \"tom_en.txt\")\n",
                "key_tom_en\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textplot_keyness(key_tom_de, key_tom_de$n_target == 1)\n",
                "textplot_keyness(key_tom_de, key_tom_en$n_target == 1)\n",
                "textplot_keyness(key_tom_de)\n",
                "textplot_keyness(key_tom_en)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 14. Razumljivost besedil\n",
                "\n",
                "Indeksi razumljivosti (readability index) so prirejeni za angleščino, za druge\n",
                "jezike veljajo v manjši meri.\n",
                "\n",
                "Flesch-Index velja angleška besedila: nižja vrednost nakazuje, da neko besedilo težje beremo (razumemo).\n",
                "\n",
                "Indeks nemškega prevoda ima nižjo vrednost (61) kot Tom Sawyer (81), kar je lahko povezano (a) z daljšimi povedmi in/ali (b) daljšimi besedami (zloženke v nemščini pišemo kot eno besedo, v angleščini pogosto ne). \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "textstat_readability(romane, measure = c(\"Flesch\", \"Flesch.Kincaid\", \"FOG\", \"FOG.PSK\", \"FOG.NRI\"))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 15. Omrežje sopojavitev (FCM)\n",
                "\n",
                "Matriko sopojavljanja besednih oblik (FCM) pridobimo v dveh korakih: \n",
                "- najprej izberemo seznam izrazov (*pattern*) iz matrike (*dfm()*), \n",
                "- potem določimo matriko sopojavljanja besednih oblik (*fcm()*).\n",
                "\n",
                "Primer omrežja iz nemškega prevoda:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dfm_tags_de <- dfm_select(matrika[1,], # tom_de.txt\n",
                "                       pattern = (c(\"tom\", \"huck\", \"*joe\", \"becky\", \"tante\",\n",
                "                                    \"witwe\",\"polly\", \"sid\", \"mary\", \"thatcher\",\n",
                "                                    \"höhle\", \"herz\",\"*schule\", \"katze\", \"geld\",\n",
                "                                    \"zaun\", \"piraten\",\"schatz\")))\n",
                "\n",
                "toptag_de <- names(topfeatures(dfm_tags_de, 50))\n",
                "head(toptag_de)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Construct feature-cooccurrence matrix (fcm) of tags\n",
                "fcm_tom_de <- fcm(matrika[1,]) # besedilo 1 je tom_de.txt\n",
                "head(fcm_tom_de)\n",
                "\n",
                "top_fcm_de <- fcm_select(fcm_tom_de, pattern = toptag_de)\n",
                "\n",
                "textplot_network(top_fcm_de, \n",
                "                 min_freq = 0.6, \n",
                "                 edge_alpha = 0.8, \n",
                "                 edge_size = 5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 16. Slovnična analiza\n",
                "\n",
                "Za slovnično analizo in lematizacijo besednih oblik lahko uporabljamo posebne\n",
                "programe (npr. *spacyr* ali *udpipe*).\n",
                "\n",
                "Program *udpipe* je na voljo za številne jezike (angleščino, nemščino, slovenščino idr.).\n",
                "\n",
                "Tu bomo ponovno uporabljali že pridobljena jezikovna modela in podatkovna niza (gl. lematizacijo), ampak tokrat za prikaz enostavnih primerov slovnične analize.\n",
                "\n",
                "## 16.1 Podatkovna niza\n",
                "\n",
                "Za lažje prepoznavo besedil bomo najprej spremenili imeni v stolpcu \"doc_id\". Potem bomo podatkovna niza združili. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "en_df = en_df %>% \n",
                "  mutate(doc_id = str_replace(doc_id, \"doc1\", \"tom_en\"))\n",
                "\n",
                "de_df = de_df %>% \n",
                "  mutate(doc_id = str_replace(doc_id, \"doc1\", \"tom_de\"))\n",
                "\n",
                "tom_df = rbind(en_df, de_df) %>% \n",
                "  mutate(token_id = as.integer(factor(token_id))) %>% \n",
                "  arrange(doc_id, paragraph_id, sentence_id, token_id)\n",
                "head(tom_df)\n",
                "tail(tom_df)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Shranjujemo in nadaljujemo naslednjič.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# write_rds(tom_df, \"data/tom_df.rds\")\n",
                "# tom_df = read_rds(\"data/tom_df.rds\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 16.2 Primerjava Noun : Pron\n",
                "\n",
                "Zdaj lahko začnemo poizvedovati po besednih oblikah, slovarskih enotah in\n",
                "slovničnih kategorijah.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tabela = tom_df %>% \n",
                "  group_by(doc_id) %>% \n",
                "  count(upos) %>% \n",
                "  filter(!is.na(upos),\n",
                "         upos != \"PUNCT\")\n",
                "head(tabela)\n",
                "\n",
                "tabela %>% \n",
                "  mutate(upos = reorder_within(upos, n, n, sep = \": \")) %>% \n",
                "  ggplot(aes(n, upos, fill = upos)) +\n",
                "  geom_col() +\n",
                "  facet_wrap(~ doc_id, scales = \"free\") +\n",
                "  theme(legend.position = \"none\") +\n",
                "  labs(x = \"Število pojavnic\", y = \"\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Izračun deležev:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "delezi = tabela %>% \n",
                "  mutate(prozent = n/sum(n)) %>% \n",
                "  pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent)\n",
                "head(delezi)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "delezi %>% \n",
                "  filter(upos %in% c(\"NOUN\", \"PRON\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ali se besedili razlikujeta glede razmerja med samostalniki in zaimki?\n",
                "Glede na to, da gre za vsebinsko in najbrž tudi slogovno zelo podobni besedili (izvirnik in prevod), in glede na to, da gre za sorodna jezika (angleščinao in nemščino), bi bila verjetna ničelna domneva (H0: med izvirnikom in prevodom ni statistično značilne razlike). Manj verjetna se zdi alternativna hipoteza (H1: med izvirnikom in prevodom je statistično značilna razlika). \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# za hi kvadrat test potrebujemo le drugi in tretji stolpec\n",
                "nominal = delezi %>% \n",
                "  filter(upos %in% c(\"NOUN\", \"PRON\")) %>% \n",
                "  select(n_tom_de, n_tom_en) \n",
                "\n",
                "# statisticni preskus\n",
                "chisq.test(nominal)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hi kvadrat test potrjuje alternativno domnevo (H1). Angleški izvirnik in nemški prevod se razlikujeta glede razmerja med samostalniki in zaimki: X\\^2 (1) = 5,71; p \\< 0,001. Iz gornje tabele pogostnosti je razvidno, da je delež samostalnikov v angleškem izvirniku nekoliko večji kot v nemškem prevodu. Razlika je sicer zaradi velikih vzorcev statistično značilna, ni pa velika, saj so deleži zelo podobni. \n",
                "\n",
                "Da bi ugotovili, ali je ugotovljena statistična značilna razlika pomembna, bi si morali podrobneje ogledati, kateri zaimki in kateri samostalniki bistveno vplivajo na to številčno razmerje. Na splošno velja, da so zaimki manj zanesljiva jezikovna sredstva kot samostalniki, samostalniki pa so bolj zapleteni.\n",
                "\n",
                "Če želimo primerjati eno besedno vrsto z vsemi drugimi v podatkovnem nizu, je\n",
                "pretvorba bolj zapletena, saj moramo - podobno kot v Excelu: \n",
                "- najprej izračunati vsoto za vse besedne vrste, \n",
                "- potem odšteti število zaimkov oz. samostalnikov od vsote, \n",
                "- razliko pa upoštevati za tabelo 2x2 za hi kvadrat test.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(zaimki = tom_df %>% \n",
                "  group_by(doc_id) %>% \n",
                "  count(upos) %>% \n",
                "  filter(!is.na(upos),\n",
                "         upos != \"PUNCT\") %>% \n",
                "  mutate(vsota = sum(n),\n",
                "         no_noun = vsota - n[upos == \"NOUN\"],\n",
                "         no_pron = vsota - n[upos == \"PRON\"]) %>% \n",
                "  filter(upos == \"PRON\") %>% \n",
                "  select(doc_id, n, no_pron) %>% \n",
                "  pivot_longer(-doc_id, 'kategorija', 'vrednost') %>%\n",
                "  pivot_wider(kategorija, doc_id)\n",
                ")\n",
                "\n",
                "(samostalniki = tom_df %>% \n",
                "  group_by(doc_id) %>% \n",
                "  count(upos) %>% \n",
                "  filter(!is.na(upos),\n",
                "         upos != \"PUNCT\") %>% \n",
                "  mutate(vsota = sum(n),\n",
                "         no_noun = vsota - n[upos == \"NOUN\"],\n",
                "         no_pron = vsota - n[upos == \"PRON\"]) %>% \n",
                "  filter(upos == \"NOUN\") %>% \n",
                "  select(doc_id, n, no_noun) %>% \n",
                "  pivot_longer(-doc_id, 'kategorija', 'vrednost') %>%\n",
                "  pivot_wider(kategorija, doc_id)\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hi kvadrat testa: \n",
                "- primerjava števila zaimkov nasproti ostalim besednim vrstam,\n",
                "- primerjava števila samostalnikov nasproti ostalim besednim vrstam.\n",
                "\n",
                "V obeh primerih spet velja:\n",
                "H0 (med vzorcema ni statistično značilne razlike). \n",
                "H1 (vzorca se značilno razlikujeta).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# izločimo prvi stolpec [, -1], \n",
                "# saj za hi kvadrat test potrebujemo le številke v drugem in tretjem stolpcu\n",
                "chisq.test(zaimki[,-1])\n",
                "chisq.test(samostalniki[,-1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Statistični izid:\n",
                "Deleža zaimkov se v besedilih ne razlikujeta (prvi test potrjuje H0), vendar pa se besedili razlikujeta glede deleža samostalnikov (drugi test potrjuje H1).\n",
                "\n",
                "\n",
                "## 16.3 Primerjava veznikov\n",
                "\n",
                "Primerjati želimo število stavkov s prirednim in podrednim veznikom.\n",
                "\n",
                "Osnovna domneva je, da priredno zložene povedi (vsebujejo stavek, uveden s\n",
                "prirednim veznikom) lažje razumemo kot podredno zložene povedi (vsebujejo\n",
                "stavek, uveden s podrednim veznikom).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(vezniki = tabela %>% \n",
                "  filter(upos %in% c(\"CCONJ\", \"SCONJ\")) %>% \n",
                "  mutate(prozent = n/sum(n)) %>% \n",
                "  pivot_wider(id_cols = upos, names_from = doc_id, values_from = n:prozent)\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Odstotki nakazujejo, da je delež prirednih veznikov v angleškem izvirniku rahlo večji kot v nemškem prevodu.\n",
                "\n",
                "Spet uporabljamo hi kvadrat test (upoštevane so le povedi, ki vsebujejo veznik) za preverjanje, ali je razlika dovolj velika, da bi bila nenaključna.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chisq.test(vezniki[,c(2:3)])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Z ozirom na hi kvadrat test razlika med besediloma ni statistično značilna (potrjen je H0).\n",
                "\n",
                "Če upoštevamo tudi vsote drugih besednih vrst (kot zgoraj):\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(koord = tabela %>% \n",
                "  mutate(vsota = sum(n),\n",
                "         no_cconj = vsota - n[upos == \"CCONJ\"],\n",
                "         no_sconj = vsota - n[upos == \"SCONJ\"]) %>% \n",
                "  filter(upos == \"CCONJ\") %>% \n",
                "  select(doc_id, n, no_cconj) %>% \n",
                "  pivot_longer(-doc_id, 'kategorija', 'vrednost') %>%\n",
                "  pivot_wider(kategorija, doc_id)\n",
                ")\n",
                "\n",
                "(subord = tabela %>% \n",
                "  mutate(vsota = sum(n),\n",
                "         no_cconj = vsota - n[upos == \"CCONJ\"],\n",
                "         no_sconj = vsota - n[upos == \"SCONJ\"]) %>% \n",
                "  filter(upos == \"SCONJ\") %>% \n",
                "  select(doc_id, n, no_sconj) %>% \n",
                "  pivot_longer(-doc_id, 'kategorija', 'vrednost') %>%\n",
                "  pivot_wider(kategorija, doc_id)\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hi kvadrat preizkus izkazuje razliko med besediloma v primeru prirednih veznikov (potrjen je H1), v primeru podrednih veznikov pa ne (potrjen je H0).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chisq.test(koord[,-1])\n",
                "chisq.test(subord[,-1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Besedili se razlikujeta glede deleža prirednih veznikov (če jih primerjamo z vsemi drugimi besednimi vrstami).\n",
                "\n",
                "\n",
                "## 16.4 Slovarske enote\n",
                "\n",
                "Program udpipe je vsako besedno obliko dodelil slovarski enoti (lemma). Koliko\n",
                "koliko slovarskih enot je v besedilih? Katerim besednim vrstam najpogosteje\n",
                "pripadajo?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(tabela2 = tom_df %>% \n",
                "  group_by(doc_id, upos) %>% \n",
                "    filter(!is.na(upos),\n",
                "           upos != \"PUNCT\",\n",
                "           upos != \"X\") %>% \n",
                "  distinct(lemma) %>% \n",
                "  count(lemma) %>% \n",
                "  summarise(lemmas = sum(n)) %>% \n",
                "  mutate(prozent = round(lemmas/sum(lemmas), 4)) %>% \n",
                "  arrange(-prozent)\n",
                ")\n",
                "\n",
                "tabela2 %>% \n",
                "  # slice_max(order_by = prozent, n=6) %>% \n",
                "  mutate(upos = reorder_within(upos, lemmas, \n",
                "                               paste(\"(\",100*prozent,\"%)\"), sep = \" \")) %>%\n",
                "  ggplot(aes(prozent, upos, fill = upos)) +\n",
                "  geom_col() +\n",
                "  facet_wrap(~ doc_id, scales = \"free\") +\n",
                "  theme(legend.position = \"none\") +\n",
                "  scale_x_continuous(labels = percent_format()) +\n",
                "  labs(x = \"Anteil\", y = \"Wortklasse\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 16.5 Korelacija besed\n",
                "\n",
                "Katere besedne pogostnosti se vzporedno povečujejo ali zmanjšujejo (pairwise\n",
                "correlation) ? Podobno analizno orodje ima tudi *Voyant Tools*.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(widyr)\n",
                "\n",
                "# pairwise correlation\n",
                "correlations = tom_df %>% \n",
                "  filter(dep_rel != \"punct\", dep_rel != \"nummod\") %>%\n",
                "  mutate(lemma = tolower(lemma), token = tolower(token),\n",
                "         lemma = str_trim(lemma), token = str_trim(token)) %>% \n",
                "  janitor::clean_names() %>%\n",
                "  group_by(doc_id, lemma, token, sentence_id) %>% \n",
                "  # add_count(token) %>% \n",
                "  summarize(Freq = n()) %>% \n",
                "  arrange(-Freq) %>% \n",
                "  filter(Freq > 2) %>% \n",
                "  pairwise_cor(lemma, sentence_id, sort = TRUE) %>% \n",
                "  filter(correlation < 1 & correlation > 0.3)\n",
                "\n",
                "head(correlations)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Tom Sawyer: Becky (dekle, ki je Tomu všeč).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correlations %>%\n",
                "  filter(item1 == \"tom\") %>%\n",
                "  mutate(item2 = fct_reorder(item2, correlation)) %>%\n",
                "  ggplot(aes(item2, correlation, fill = item2)) +\n",
                "  geom_col(show.legend = F) +\n",
                "  coord_flip() +\n",
                "  labs(title = \"What tends to appear with 'Becky'?\",\n",
                "       subtitle = \"Among elements that appeared in at least 2 sentences\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 17. Sentiment\n",
                "\n",
                "Stopnjo čustvenosti ali emocionalnosti besedila je mogoče določiti s\n",
                "sentimentnim slovarjem.\n",
                "\n",
                "## 17.1 Različica 1\n",
                "\n",
                "Uporaba nrc leksikona za nemščino (priložen programu syuzhet).\n",
                "\n",
                "Najprej besedilo s funkcijo *get_sentences()* razcepimo na povedi.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(syuzhet)\n",
                "\n",
                "tom_v = get_sentences(txt$text[1]) # izberemo prvo besedilo: tom_de.txt\n",
                "tom_v = (tom_v[-1]) # tako lahko izločimo prvo vrstico (uredniško pripombo)\n",
                "head(tom_v[-1])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Funkcija *get_sentiment()* dodeli besedam v povedih pozitivno (+1), negativno\n",
                "(-1) ali nevtralno (0) čustveno vrednost. Program te vrednosti sešteje.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tom_values <- get_sentiment(tom_v, method = \"nrc\", language = \"german\")\n",
                "length(tom_values)\n",
                "tom_values[100:110]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Povedi, čustvene vrednosti in dolžino povedi povežemo v podatkovni niz. To nam\n",
                "olajšuje oceno, kako uspešna je bila uporaba sentimentnega slovarja v našem\n",
                "besedilu. Preimenovali bomo tudi nekaj stolpcev.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment1 = cbind(tom_v, tom_values, ntoken(tom_v)) %>% \n",
                "  as.data.frame() %>% \n",
                "  rename(words = V3,\n",
                "         text = tom_v,\n",
                "         values = tom_values) %>% \n",
                "  mutate(doc_id = \"tom_de.txt\") %>% \n",
                "  rowid_to_column(var = \"sentence\")\n",
                "head(sentiment1)\n",
                "View(sentiment1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Gornje postopke ponovimo za besedilo, ki ga želimo primerjati s prvim.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "prozess_v = get_sentences(txt$text[2]) # izberemo drugo besedilo: tom_en.txt\n",
                "prozess_v = (prozess_v[-1]) # tako lahko izločimo prvo vrstico (uredniško pripombo)\n",
                "prozess_values <- get_sentiment(prozess_v, method = \"nrc\", language = \"english\")\n",
                "sentiment2 = cbind(prozess_v, prozess_values, ntoken(prozess_v)) %>% \n",
                "  as.data.frame() %>% \n",
                "  rename(words = V3,\n",
                "         text = prozess_v,\n",
                "         values = prozess_values) %>% \n",
                "  mutate(doc_id = \"tom_en.txt\") %>% \n",
                "  rowid_to_column(var = \"sentence\")\n",
                "head(sentiment2)\n",
                "View(sentiment2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "S seštevanjem čustvenih vrednosti je mogoče oceniti, katero besedilo ima več\n",
                "pozitivno ocenjenih besed. V ta namen bomo združili podatkovna niza in uredili\n",
                "obliko stolpcev \"words\" in \"values\".\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment = rbind(sentiment1, sentiment2) %>% as_tibble() %>% \n",
                "  mutate(values = parse_number(values),\n",
                "         words = parse_number(words)) %>%\n",
                "  select(doc_id, sentence, words, values, text)\n",
                "head(sentiment)\n",
                "tail(sentiment)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Rezultat: po gornji metodi je povprečje čustvenih vrednosti v nemškem prevodu\n",
                "rahlo manjše kot v angleškem izvirniku \"Tom Sawyer\", vendar je razlika tako majhna, da najbrž ne bi bila statistično značilna. Povprečje je v obeh primerih blizu nevtralne vrednosti (tj. 0): Tom Sawyer vsebuje kar nekaj vedrih prigod in dogodivščin, je pa res, da so njegove pustolovščine pogosto tudi nevarne ali strašljive.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment %>% \n",
                "  group_by(doc_id) %>% \n",
                "  summarise(polarnost = mean(values))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Poskusimo še drugače: pozitivne, nevtralne in negativne vrednosti obravnajmo ločeno in upoštevajmo tudi dolžino povedi.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment1 = sentiment %>% \n",
                "  group_by(doc_id) %>% \n",
                "  mutate(positive = ifelse(values > 0, abs(values), 0),\n",
                "         neutral = ifelse(values == 0, 1, 0),\n",
                "         negative = ifelse(values < 0, abs(values), 0))\n",
                "sentiment1 %>% \n",
                "  summarise(pos = mean(100*positive/words),\n",
                "            neut = mean(100*neutral/words),\n",
                "            neg = mean(100*negative/words))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ta rezultat nakazuje, so čustvene vrednosti v nemškem prevodu nekoliko skrajnejše (pozitivne ali negativne) kot v angleškem izvirniku. Zanimivo bi bilo vprašati poznavalca angleškega izvirnika in nemškega prevoda, ali je ob slogovni primerjavi dobil podoben vtis. \n",
                "\n",
                "Poglejmo še nekaj povedi, ki so bile ocenjene negativno:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sentiment1 %>% \n",
                "  filter(negative > 0)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 17.2 Različica 2\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tom_v = get_sentences(txt$text[2]) # angleški izvirnik\n",
                "tom_nrc_values = get_nrc_sentiment(tom_v)\n",
                "tom_joy_items = which(tom_nrc_values$joy > 0)\n",
                "head(tom_v[tom_joy_items], 4)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nrc_sentiment = as.data.frame(cbind(tom_v, tom_nrc_values))\n",
                "head(nrc_sentiment) %>% paged_table()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 17.3 Različica 3\n",
                "\n",
                "Drugi sentimentni slovarji z medmrežja: npr. BAWLR lahko uporabljamo kot\n",
                "sentimentni slovar.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This lexicons contains values of Emotional valence and arousal ranging from 1 to 5.\n",
                "# But this extended version contains also binary Emo_Val values (1, -1).\n",
                "bawlr <- read.delim2(\"data/BAWLR_utf8.txt\", sep = \"\\t\", dec = \",\", fileEncoding = \"UTF-8\", \n",
                "                     header = T, stringsAsFactors = T)\n",
                "# # bawlr$EmoVal <- as.character(bawlr$EmoVal)\n",
                "# # str(EmoVal)\n",
                "# bawlr$EmoVal <- gsub('NEG', '-1', bawlr$EmoVal)\n",
                "# bawlr$EmoVal <- gsub('POS', '1', bawlr$EmoVal)\n",
                "# bawlr$EmoVal <- as.numeric(bawlr$EmoVal)\n",
                "head(bawlr)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Sestavimo dva seznama:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "positive.words = bawlr %>% \n",
                "  mutate(WORD_LOWER = as.character(WORD_LOWER)) %>% \n",
                "  select(EmoVal, WORD_LOWER) %>% \n",
                "  filter(EmoVal == \"POS\") %>% \n",
                "  select(WORD_LOWER) %>% \n",
                "  filter(str_detect(WORD_LOWER, \"[a-zA-Z]\"))\n",
                "\n",
                "negative.words = bawlr %>% \n",
                "  mutate(WORD_LOWER = as.character(WORD_LOWER)) %>% \n",
                "  select(EmoVal, WORD_LOWER) %>% \n",
                "  filter(EmoVal == \"NEG\") %>% \n",
                "  select(WORD_LOWER) %>% \n",
                "  filter(str_detect(WORD_LOWER, \"[a-zA-Z]\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Ustvarimo quanteda slovar *dictionary()*:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bawlr_dict = dictionary(list(positive = list(positive.words), negative = list(negative.words)))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Quanteda slovar lahko shranimo na disk.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# jsonlite::write_json(bawlr_dict, \"data/quanteda_bawlr_dict.json\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Uporabljamo matriko (dfm) s slovarskimi enotami (lemma), saj slovar bawlr_dict\n",
                "vsebujejo le osnovno obliko slovarskih enot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "matrika_lemmas = dfm(matrika_lem_de, tolower = TRUE)\n",
                "\n",
                "result = matrika_lemmas %>% \n",
                "  dfm_lookup(bawlr_dict) %>% \n",
                "  convert(to = \"data.frame\") %>% \n",
                "  as_tibble\n",
                "result\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Dodamo lahko skupno dolžino besed, če želimo normalizirati rezultat z ozirom na\n",
                "dolžino besedil.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = result %>% mutate(length=ntoken(matrika_lemmas))\n",
                "result\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Po navadi želimo izračunati skupno sentimentno vrednost. Možnosti je več: npr. -\n",
                "odšteti negativne vrednosti od pozitivnih in nato razliko deliti z vsoto obeh\n",
                "vrednosti, - odšteti negativne vrednosti od pozitivnih in nato razliko deliti z\n",
                "dolžino besedil,\n",
                "\n",
                "Izračunamo lahko tudi stopnjo subjektivnosti, tj. koliko čustvenih vrednosti je\n",
                "skupno izraženih:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "result = result %>% mutate(sentiment1=(positive - negative) / (positive + negative))\n",
                "result = result %>% mutate(sentiment2=(positive - negative) / length)\n",
                "result = result %>% mutate(subjektivnost=(positive + negative) / length)\n",
                "result %>% paged_table()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Barvno označevanje\n",
                "\n",
                "Program corpustools barvno označuje besede v besedilih z ozirom na čustvene\n",
                "vrednosti besed v sentimentnem slovarju.\n",
                "\n",
                "Prvi korak je ustvarjanje tcorpusa.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(corpustools)\n",
                "t = create_tcorpus(txt1, doc_column=\"doc_id\") # izbrali smo le nemški prevod\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "V drugem koraku sledi iskanje po slovarju (tcorpus):\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t$code_dictionary(bawlr_dict, column = 'bawlr')\n",
                "t$set('sentiment', 1, subset = bawlr %in% c('positive','neg_negative'))\n",
                "t$set('sentiment', -1, subset = bawlr %in% c('negative','neg_positive'))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Prikaz barvno označenih besedil v oknu \"Viewer\":\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "browse_texts(t, scale='sentiment')\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Prikaz barvno označenih besedil v spletnem brskalniku in shranjevanje v obliki\n",
                "html datoteke:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "browse_texts(t, scale='sentiment', filename = \"sentiment_tom.html\", \n",
                "             header = \"Sentiment in Twains Tom Sawyer\")\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
